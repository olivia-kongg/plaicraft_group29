{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4ace2b-5d3d-4c36-843d-afeaac89a4cd",
   "metadata": {},
   "source": [
    "Project Planning\n",
    "---\n",
    "Olivia Kong, Student ID: 72594369\n",
    "Group 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59544b93-23c3-47f1-b25b-8400db858905",
   "metadata": {},
   "source": [
    "Question\n",
    "-\n",
    "Question 3: We would like to know something about our populations of users, in particular, we would like to have a good model of whether or not a player will continue contributing given past participation. \n",
    "\n",
    "Group 29 has decided to study Question 3. The goal of this project is to predict player retention, whether a player will continue playing the game based on the data from the two given dataframes. This is a classification prediction problem where new observations will be assigned a class (for example: keeps playing or doesn't keep playing). The two dataframes, 'players.csv' and 'sessions.csv', have a few  variables that will help us to classify new observations, therefore helping us towards an answer for this research question. Specifically, we will use these variables as predictors when predicting the class for new observations. \n",
    "New observations will be organized as a class and not a numerical value. This will require a KNN classification model with the most optimal K value, and useful predictor variables. The data will also be split into training and test sets to evaluate the classifer model's performance using accuracy, recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15393479-decf-4fd9-95ed-6527537ee392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "\n",
    "\n",
    "\n",
    "alt.data_transformers.enable('vegafusion')\n",
    "\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b260f3c-552e-4b76-9f91-7cbe1d1961d3",
   "metadata": {},
   "source": [
    "Players Messy Dataframe\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a1142b-3c70-4517-abd5-907f46f069c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'players.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m players_messy\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplayers.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m players_messy\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'players.csv'"
     ]
    }
   ],
   "source": [
    "players_messy=pd.read_csv('players.csv')\n",
    "players_messy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94b591-0dc9-4b37-95dc-d5890052f2a4",
   "metadata": {},
   "source": [
    "This dataframe shows the data from the players.csv file, which includes a list about the players and data about each player. Variables include experience, hours played, age and gender. These variables can be plotted to find any trends or patterns in our data set, which can help to classify any new observations. This is because these columns can be used as predictor variables to predict the class of new observations in the dataframe. There are some columns in this data set that will not be very useful in this project, such as the hashed email, organization name and individual ID. In fact, the columns 'organizationName' and 'individualId' don't have any values for any observations. These three columns are not useful, because the values in these columns aren't applicable or relevant to our research question, therefore they can be dropped. \n",
    "\n",
    "\n",
    "There are 196 rows, which corresponds to 196 observations and there are 9 columns, each representing a variable. However, not all columns are necessarily useful variables, therefore they could be dropped during data wrangling, as mentioned above. Additionally, the columns contain a mix of data types, with some being numerical and others categorical.\n",
    "\n",
    "Numerical columns: played_hours, age\n",
    "\n",
    "Categorical columns: experience, subscribe, gender \n",
    "\n",
    "\n",
    "Note, the name column is not a categorical column because it simply acts to identify the players, it doesn't provide any predictive value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05854dcb-1a01-4fc1-920c-3ba1c2b42471",
   "metadata": {},
   "source": [
    "Sessions messy dataframe\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d1cd4-f9a7-4edc-b8cd-6015a1601e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions=pd.read_csv('sessions.csv')\n",
    "sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76103f09-918f-44f3-ae9b-5ab2f0c8ff88",
   "metadata": {},
   "source": [
    "This dataframe shows the data from the sessions.csv file, which includes the start and stop time for each player, however compared to the players dataframe, only the 'hashedEmail' column is present to help identify each player (no names, gender, etc.). Therefore, it would be helpful to merge the two dataframes on the 'hashedEmail' column, as this would help find more patterns and trends in the data. For example, if pro players tend to play around the same time in the day, this information would be hard to find by looking at the two datasets separately. You would first need to identify the time in the sessions dataframe, try to match the hashed email on both data frames, and then find the player's experience class on the players dataframe. Therefore, this sessions dataframe doesn't offer as many potential predictor variables as the players dataframe, only the start and end time from the sessions dataframe would likely be used. Once again, the hashed email column doesn't provide any insight to the classification of new observations, it shouldn't be used as a predictor variable, however it can be used to merge the two dataframes together, as mentioned before.\n",
    "\n",
    "The original start and end time columns are hard to interpret, as the values in these columns are really big (~1.7 trillion). Compared to the 'start_time' and 'end_time' columns that include the date and time, the 'original_start_time' and 'original_end_time' only provide a really big number. From external research, the 'original_start_time' and 'original_end_time' columns show Unix time which represents the number of seconds that have elapsed since January 1 1970. Since the 'start_time' and 'stop_time' columns are included in the sessions dataframe, there is no need for the original start and stop time, as it's not in a readable format. \n",
    "Therefore the columns hashedEmail, and the original start and stop time are not useful for the classification of new observations.\n",
    "\n",
    "This dataframe contains 1535 rows corresponding to 1535 observations, and 5 columns each representing a variable. There are more rows in this data frame compared to the players dataframe that only has 196 rows. This is likely because each session is recorded as an observation, so if a player has more than one session, this would show up as multiple observations in the sessions dataframe. Once again, not every column can be used as a predictor variable as mentioned above, therefore they can be dropped during data wrangling. \n",
    "\n",
    "Looking at the 'start_time' and 'end_time' columns, there are two values in a single cell, which isn't very tidy. This could be improved by splitting these columns into two separate columns, for instance, one for time and one for date. This can be done using the str.split function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058791d-35b2-4c6b-b25b-9e29df2133f8",
   "metadata": {},
   "source": [
    "Tidying the Data Frames\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989da3a-75e2-4352-856e-905138285194",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions[['start_date','start_time']]=sessions['start_time'].str.split(' ', expand=True)\n",
    "sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8aa8c-03fd-4dae-8fb2-7e0a0d1b9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions[['end_date','end_time']]=sessions['end_time'].str.split(' ',expand=True)\n",
    "sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18f946-da67-445f-b2ce-f9ca20f2d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_tidy=sessions.drop(columns=['original_start_time','original_end_time'])\n",
    "sessions_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be28d4-3430-4e9e-85bc-c68e5303c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_tidy[\"start_time\"] = pd.to_datetime(sessions[\"start_time\"], format = \"%H:%M\")\n",
    "sessions_tidy[\"start_time\"] = sessions_tidy[\"start_time\"].dt.time\n",
    "#sessions_tidy[\"session_time\"] = sessions_tidy[\"start_time\"].dt.minute /60\n",
    "sessions_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ea42b-d281-4c0b-b1f7-af43e89e51bf",
   "metadata": {},
   "source": [
    "Players dataframe merged with Sessions dataframe on hashed email\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3a7ae-fbd2-4d87-af91-0b0ae828d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_session_tgt=sessions_tidy.merge(players_messy, on='hashedEmail')\n",
    "players_session_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c58b9-2282-45de-b73d-6ac85e421bc7",
   "metadata": {},
   "source": [
    "Players_session_tgt Dataframe, dropping unhelpful columns\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a487b5-2964-4d8d-8a53-404d94ebc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_session_tgt=players_session_tgt.drop(columns=['hashedEmail','individualId','organizationName', 'start_date', 'end_date'])\n",
    "players_session_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae61be-1b00-4d1c-8ee9-c4cf10934920",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_order = ['name', 'start_time', 'start_date', 'end_time', 'end_date', 'experience','subscribe','gender','played_hours','age']\n",
    "\n",
    "\n",
    "players_session_tgt = players_session_tgt[new_column_order]\n",
    "players_session_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6187a-caf2-40b6-9d87-9ae760b705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "age_vs_hours_plot=alt.Chart(players_session_tgt,title='Experience: Hours Played vs Age').mark_circle(clip=True).encode(\n",
    "    x=alt.X('age').title('Age').scale(domain=['6','30'],zero=False),\n",
    "    y=alt.Y('played_hours').title('Hours Played').scale(domain=['0','6'],zero=False),\n",
    "    color=alt.Color('experience').title('Experience')\n",
    ")\n",
    "age_vs_hours_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd7ff1-829b-4935-945b-e1bbe66cf0f5",
   "metadata": {},
   "source": [
    "The plot of hours played versus age shows significant overplotting, with most observations concentrated around age 20 and approximately 1 hour of playtime. Even when narrowing the x-axis domain (age) to examine the spacing of the points, the points remain densely packed, preventing any patterns or relationships from being observed. Color was added to the plot based on the experience column to help identify potential patterns or trends, but due to the overplotting, no conclusions could be extracted from this feature. The clustering of points can be attributed to the fact that the majority of observations come from the same age group (15-20), causing data from these individuals to overlap in the same region. In order for there to be any type of relationship (positive, negative, weak, strong), there would need to be a more broader distrubution of observations from each age. It can also be observed that many points remain on the x-axis (0 hours of playtime),making it harder to spot trends or patterns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d567b49-ab9c-4ebe-b644-896efbcd02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_vs_hours_plot_2=alt.Chart(players_session_tgt, title='Gender: Hours Played vs Age').mark_circle(clip=True).encode(\n",
    "    x=alt.X('age').title('Age').scale(domain=['6','30'],zero=False),\n",
    "    y=alt.Y('played_hours').title('Hours Played').scale(domain=['0','7'],zero=False),\n",
    "    color=alt.Color('gender').title('Gender').scale(scheme='set1')\n",
    ")\n",
    "age_vs_hours_plot_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a14366-1ac0-40f0-af93-fa4b2fbcb664",
   "metadata": {},
   "source": [
    "This shows the same plot as above, except the 'Gender' column was used for the color function instead of 'Experience'. There isn't much difference, as the plots are still very condensed in the same area (age 20 and 1 hour of playtime), therefore no major trends or patterns can be observed. In this plot, I limited the domain once again, to zoom in closer to the points, and by doing so, it can be observed that there are many more green points than any other colored points. This corresponds to male players, therefore this could be helpful later one when deciding on predictor variables to use to classify new observations. In other words, from this plot we notice that there are more male players contributing to the data, therefore we could hypothesize that male players are more likely to keep playing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10068e64-e468-41f9-ae33-fd24f35c1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_vs_hours_plot_3=alt.Chart(players_session_tgt, title='Subscribe: Hours Played vs Age').mark_circle(clip=True).encode(\n",
    "    x=alt.X('age').title('Age').scale(domain=['6','30'],zero=False),\n",
    "    y=alt.Y('played_hours').title('Hours Played').scale(domain=['0','7'],zero=False),\n",
    "    color=alt.Color('subscribe').title('Subscribed (Yes or No)').scale(scheme='set1')\n",
    ")\n",
    "age_vs_hours_plot_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bc71f-e3c2-4b96-8e3b-0cf2e175241a",
   "metadata": {},
   "source": [
    "Again, this is the same plot as the two above, however the 'subscribe' column is used in the color function. By zooming into the region of overplotting, we can see the distribution of the point slightly better. It can be observed that there are more blue points than red points, meaning more players are subscribed than not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394e4bc-9458-46bd-836c-10997d8717cc",
   "metadata": {},
   "source": [
    "Methods and Plan\n",
    "-\n",
    "Question 3: We would like to know something about our populations of users, in particular, we would like to have a good model of whether or not a player will continue contributing given past participation. \n",
    "\n",
    "\n",
    "This question is a classification question, therefore a class will be predicted for new observations instead of a numerical value as in regression. As mentioned above, there are a few potential predictor variables that can be used to predict whether a player will continue playing, such as 'experience','gender' and 'subscribe'. Scatter plots were created above, examining these few predictor variables, however no major relationships could be determined. To address this question, a KNN classifier will be created and trained on the training set, with the most optimal k-value determined through cross-validation. \n",
    "A possible way of doing this, is to first split the data into a training and testing set (e.g. 75% training set, 25% testing set), creating a GridSearchCV object from the KNN classifier, selected parameters and chosen amount of folds.  By fitting the GridSearchCV object, we can evaluate cross-validation accuracy for each k-value and create a separate dataframe with these results using the 'cv_results_' attribute. We can plot this separate dataframe as a line plot to see which K value is the most accurate and optimal. Now that the optimal K value had been determined, we can create and retrain a new classifier with this value, and we can evaluate it's ability to correctly predict observations by making a confusion matrix. The confusion matrix will allow us to evaluate the model's accuracy, precision and recall. \n",
    "\n",
    "When determining the most optimal K value, we are doing so be observing which K value has the highest cross-validation accuracy on the line plot, but it is possible that a range of numbers would be perform similarly. This is because the values we see on the plot are estimates of the true accuracy of the classifier model. Therefore, the highest K value on the line plot doesn't necessarily mean that the classifier is the most accurate with this K value. Therefore, during this step in the project, we will assume that the most accurate K value estimated on our accuracy plot is the most optimal. Additionally, since we will be using a KNN classification model, this means that the KNN model will assume that the closer points are, the more related and similar they will be. This might be problematic because of the overplotting of the points in the relatively small dataset, because it becomes difficult to differenciate between classes based on proximity. In other words, it might be difficult for the classification model to accurately determine which class an observation belongs to based on its closest neighbors. Additionally, the smaller dataset means that the KNN model will have less information to learn any patterns and the overplotting makes this even worse by reducing the variation in the data points. \n",
    "\n",
    "A KNN classification model should work fine for this classification problem, because it is capable of working for multi-class classification problems and we are only aiming to classify if a player will continue playing (yes or no). One of the limitations of a KNN model is that it doesn't work as quickly with a larger dataset, however as mentioned above, the dataset is relatively small, therefore this shouldn't be too much of an issue. Another limitation is that it might not perform as well with multiple predictor variables, and this might be an issue since we are likely using 3 predictor variables (experience, gender and subscribed). Additionally, if the classes are imbalanced, this may impact the performance of the model, and we can potentially see this coming up by looking at the plots above. For example, from the plot above where the subscribe column is used for the color function, we see more blue points than red, therefore this may suggest that the clases are imbalanced. To address this, we can use the sample function which allows us to increase the number of rare observations to equal the amount of frequent observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d173d-2cfe-4c2c-a242-f8ae4a10a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477dae20-d2c8-4d72-9359-6418566c8193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
